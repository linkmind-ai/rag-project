{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c3ea616-282b-4396-a1e7-3ccd19a191d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement google-generative-ai (from versions: none)\n",
      "ERROR: No matching distribution found for google-generative-ai\n"
     ]
    }
   ],
   "source": [
    "!pip install notion-client langchain langchain-community chromadb sentence-transformers \\\n",
    "            google-generative-ai langchain-google-genai -q\n",
    "!pip install --upgrade --quiet google-generativeai langchain-google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a23cbb96-3edc-41df-bb49-24bb1ed3d2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "6f367f31-ca06-45f5-b8e1-248846542dbc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35f76081-c8fc-4b36-95ad-0427645aa89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Notion 블록을 로드 중...\n",
      "    → 로드된 블록 개수: 127\n",
      "🔄 블록 합쳐서 텍스트 길이: 3273\n",
      "🔄 임베딩 모델 초기화 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yss63\\AppData\\Local\\Temp\\ipykernel_13212\\3239570107.py:87: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = SentenceTransformerEmbeddings(model_name=\"jhgan/ko-sbert-sts\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Chroma 벡터스토어에 저장 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yss63\\AppData\\Local\\Temp\\ipykernel_13212\\3239570107.py:95: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 벡터 DB 저장 완료: ./vectordb\n",
      "\n",
      "▶ 질의를 입력하세요 (exit 입력 시 종료)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문:  이 문서에 대해 요약해줘\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏳ 답변 생성 중…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yss63\\AppData\\Local\\Temp\\ipykernel_13212\\3239570107.py:110: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=VECTOR_DB_DIR, embedding_function=embedding)\n",
      "C:\\Users\\yss63\\AppData\\Local\\Temp\\ipykernel_13212\\3239570107.py:112: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💡 답변:\n",
      "이 문서는 RAG(Retrieval-Augmented Generation) 기술의 발전 과정과 구성 요소, 그리고 관련된 다양한 최적화 기법들을 체계적으로 분석하고 있습니다.\n",
      "\n",
      "**1. RAG 기술 발전 단계:**\n",
      "\n",
      "*   **1단계:** Transformer 모델과 외부 지식 결합을 통한 성능 향상\n",
      "*   **2단계:** ChatGPT 등장으로 LLM의 In-Context Learning 능력 강화 및 RAG를 통한 복잡한 질의 응답\n",
      "*   **3단계:** RAG와 LLM의 구조적 결합 및 파인튜닝 통합\n",
      "\n",
      "**2. RAG 개요:**\n",
      "\n",
      "*   **기본 원리:** Indexing -> Retrieval -> Generation 의 3단계로 구성.\n",
      "*   **Naive RAG:** 간단한 \"Retrieve-Read\" 파이프라인이지만 단점 존재.\n",
      "*   **Advanced RAG:** 검색 품질 개선을 위한 pre-retrieval 및 post-retrieval 최적화 적용.\n",
      "*   **Modular RAG:** 새로운 모듈과 상호작용 패턴을 추가하여 유연성 및 맞춤화 강화.\n",
      "*   **RAG vs Fine-tuning:** RAG는 새로운 데이터 처리 및 Fine-tuning과 결합 시 성능 향상.\n",
      "\n",
      "**3. Retrieval (검색) 주요 내용:**\n",
      "\n",
      "*   **Retrieval Source:** 다양한 형태의 데이터 (비정형, 반정형, 구조화, LLM 생성 콘텐츠) 활용\n",
      "*   **Retrieval Granularity:** 사용자 Prompt에 따라 검색 단위 세분성 설정 중요 (정확도 향상).\n",
      "*   **Indexing Optimization:** 청크 분할 전략, 메타데이터 부착, 구조적 인덱스 구축을 통해 검색 효과 향상.\n",
      "*   **Query Optimization:** 질의 확장, 변형, 라우팅을 통해 Naive RAG의 문제점 개선.\n",
      "*   **Embedding:** 임베딩 모델의 의미 표현 능력 중요, 혼합 검색 및 임베딩 모델 파인튜닝 활용.\n",
      "*   **Adapter:** 모델 파인튜닝 시 발생하는 문제 해결을 위해 외부 어댑터 도입.\n",
      "\n",
      "**4. Generation (생성) 및 Augmentation (확장):**\n",
      "\n",
      "*   **Context Curation:** Reranking (재정렬), Context Selection & Compression 등을 통해 LLM 입력 최적화.\n",
      "*   **Augmentation:** Iterative, Recursive, Adaptive Retrieval 방식을 활용하여 복잡한 추론 작업 성능 향상.\n",
      "\n",
      "**결론적으로, 이 문서는 RAG 기술의 발전, 구성 요소, 그리고 성능 향상을 위한 다양한 방법론을 종합적으로 제시하며, LLM과의 통합을 강조하고 있습니다.**\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 120\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m▶ 질의를 입력하세요 (exit 입력 시 종료)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m질문: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m q\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m종료합니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# local_rag_gemini.py\n",
    "\n",
    "import os\n",
    "from notion_client import Client\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import google.generativeai as genai\n",
    "\n",
    "# ─── 사용자 설정 ─────────────────────────────────────────────\n",
    "NOTION_API_KEY = \"ntn_S49134845636QN7OizYlyythCTORUXOCvYcp2U19S0P6dy\"        # 예: ntn_...\n",
    "PAGE_ID          = \"1dd124ddd31380acb247eb0c791331d9\"                       # ?pvs=11 제거된 순수 페이지 ID\n",
    "VECTOR_DB_DIR    = \"./vectordb\"                         # 벡터 DB가 저장될 디렉토리\n",
    "GOOGLE_API_KEY   = \"AIzaSyDSQvjXRYlqA6nYdCSX8l6WmoZxXV0aHMo\"                # Google Generative AI 키\n",
    "GEMINI_MODEL     = \"gemini-2.0-flash\"                   # or \"gemini-1.5-pro\"\n",
    "GENAI_TEMP       = 1.0                                  # 생성 온도\n",
    "\n",
    "# ─── Notion 클라이언트 설정 ─────────────────────────────────\n",
    "os.environ[\"NOTION_API_KEY\"] = NOTION_API_KEY\n",
    "notion = Client(auth=NOTION_API_KEY)\n",
    "\n",
    "# ─── Google Generative AI 설정 ───────────────────────────────\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "# (LangChain 래퍼는 쓰지 않고, 직접 genai.GenerativeModel 호출)\n",
    "\n",
    "# ─── 1) Notion 페이지 블록 전부 가져오기 ───────────────────────\n",
    "def fetch_all_blocks(page_id: str) -> list[dict]:\n",
    "    all_blocks = []\n",
    "    cursor = None\n",
    "    while True:\n",
    "        resp = notion.blocks.children.list(\n",
    "            block_id=page_id,\n",
    "            start_cursor=cursor,\n",
    "            page_size=100\n",
    "        )\n",
    "        all_blocks.extend(resp[\"results\"])\n",
    "        if not resp.get(\"has_more\"):\n",
    "            break\n",
    "        cursor = resp.get(\"next_cursor\")\n",
    "    return all_blocks\n",
    "\n",
    "# ─── 2) 블록에서 텍스트만 뽑아내기 ────────────────────────────\n",
    "def block_to_text(b: dict) -> str:\n",
    "    t = \"\"\n",
    "    typ = b[\"type\"]\n",
    "    rich = b[typ].get(\"rich_text\", [])\n",
    "    # 단순 paragraph / heading / list / code 처리\n",
    "    if typ in (\"paragraph\", \"heading_1\", \"heading_2\", \"heading_3\"):\n",
    "        for r in rich:\n",
    "            t += r.get(\"plain_text\", \"\")\n",
    "        if typ.startswith(\"heading\"):\n",
    "            t = \"\\n\" + t.upper() + \"\\n\"\n",
    "    elif typ == \"bulleted_list_item\":\n",
    "        for r in rich:\n",
    "            t += r.get(\"plain_text\", \"\")\n",
    "        t = \"- \" + t\n",
    "    elif typ == \"numbered_list_item\":\n",
    "        for r in rich:\n",
    "            t += r.get(\"plain_text\", \"\")\n",
    "        t = \"1. \" + t\n",
    "    elif typ == \"code\":\n",
    "        lang = b[\"code\"][\"language\"]\n",
    "        for r in rich:\n",
    "            t += r.get(\"plain_text\", \"\")\n",
    "        t = f\"```{lang}\\n{t}\\n```\"\n",
    "    return t\n",
    "\n",
    "# ─── 3) 페이지 하나를 LangChain Document로 묶기 ───────────────────\n",
    "print(\"🔄 Notion 블록을 로드 중...\")\n",
    "blocks = fetch_all_blocks(PAGE_ID)\n",
    "print(f\"    → 로드된 블록 개수: {len(blocks)}\")\n",
    "\n",
    "full_text = \"\\n\".join(\n",
    "    txt for blk in blocks\n",
    "    if (txt := block_to_text(blk).strip())\n",
    ")\n",
    "print(f\"🔄 블록 합쳐서 텍스트 길이: {len(full_text)}\")\n",
    "\n",
    "docs = [Document(page_content=full_text, metadata={\"source\": PAGE_ID})]\n",
    "\n",
    "# ─── 4) SBERT 임베딩 + Chroma DB 생성 ───────────────────────────\n",
    "print(\"🔄 임베딩 모델 초기화 중...\")\n",
    "embedding = SentenceTransformerEmbeddings(model_name=\"jhgan/ko-sbert-sts\")\n",
    "\n",
    "print(\"🔄 Chroma 벡터스토어에 저장 중...\")\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding,\n",
    "    persist_directory=VECTOR_DB_DIR\n",
    ")\n",
    "vectordb.persist()\n",
    "print(\"✅ 벡터 DB 저장 완료:\", VECTOR_DB_DIR)\n",
    "\n",
    "# ─── 5) Gemini를 이용한 RetrievalQA 헬퍼 ───────────────────────\n",
    "def generate_gemini_answer(query: str, context: str) -> str:\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "    model = genai.GenerativeModel(\n",
    "        GEMINI_MODEL,\n",
    "        generation_config=genai.GenerationConfig(temperature=GENAI_TEMP)\n",
    "    )\n",
    "    return model.generate_content(prompt).text\n",
    "\n",
    "\n",
    "def ask_question(q: str) -> str:\n",
    "    # 최신 DB 로드\n",
    "    db = Chroma(persist_directory=VECTOR_DB_DIR, embedding_function=embedding)\n",
    "    retriever = db.as_retriever()\n",
    "    docs = retriever.get_relevant_documents(q)\n",
    "    context = \"\\n\\n\".join(d.page_content for d in docs)\n",
    "    return generate_gemini_answer(q, context)\n",
    "\n",
    "# ─── 6) 대화형 루프 ─────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n▶ 질의를 입력하세요 (exit 입력 시 종료)\")\n",
    "    while True:\n",
    "        q = input(\"질문: \").strip()\n",
    "        if q.lower() in (\"exit\",\"quit\"):\n",
    "            print(\"종료합니다.\")\n",
    "            break\n",
    "        print(\"\\n⏳ 답변 생성 중…\")\n",
    "        ans = ask_question(q)\n",
    "        print(\"\\n💡 답변:\")\n",
    "        print(ans)\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bb8dc36-7d15-48f1-ab8e-c58d190571e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ 질의를 입력하세요 (exit 입력 시 종료)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문:  이 문서에 대해 요약해줘\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏳ 답변 생성 중…\n",
      "\n",
      "💡 답변:\n",
      " ## RAG 기술 발전 및 LLM 통합 연구 논문 요약\n",
      "\n",
      "**1. 서론:**\n",
      "\n",
      "*   본 논문은 RAG(Retrieval-Augmented Generation) 기술의 발전 과정을 체계적으로 정리하고, LLM(Large Language Model)과의 통합을 중심으로 기술 패러다임과 연구 방법론을 분석합니다.\n",
      "*   RAG 기술은 Transformer 모델의 등장으로 시작되어 ChatGPT와 같은 LLM의 발전과 함께 고도화되었으며, 현재는 단순 검색 보조를 넘어 LLM 파인튜닝 및 구조적 결합을 통해 더욱 발전하고 있습니다.\n",
      "\n",
      "**2. RAG 개요:**\n",
      "\n",
      "*   RAG는 질문에 답변하기 위해 외부 지식을 활용하는 방식으로, 일반적으로 1) Indexing, 2) Retrieval, 3) Generation의 세 단계를 거칩니다.\n",
      "*   RAG는 Naive RAG에서 Advanced RAG, Modular RAG로 발전해왔으며, Fine-Tuning(FT)과 결합하여 성능을 더욱 향상시킬 수 있습니다. RAG는 특히 새로운 데이터나 보지 못한 지식을 처리하는 데 FT보다 우수한 성능을 보입니다.\n",
      "\n",
      "**3. Retrieval:**\n",
      "\n",
      "*   Retrieval 단계는 데이터 소스로부터 관련 문서를 효율적으로 검색하는 것이 핵심입니다.\n",
      "*   **검색 소스 (Retrieval Source):** RAG는 LLM을 보완하기 위해 외부 지식에 의존하며, 데이터 구조는 비정형, 반구조화, 구조화 데이터 및 LLM 생성 콘텐츠를 포함합니다. 검색 단위 세분성은 사용자 Prompt에 따라 설정하여 검색 정확도를 높입니다.\n",
      "*   **인덱싱 최적화 (Indexing Optimization):** 문서 전처리, 분할(Chunking), 임베딩 변환 후 벡터 DB에 저장하는 과정을 통해 검색 효과를 향상시킵니다. 청크 분할 전략, 메타데이터 부착, 구조적 인덱스 등이 활용됩니다.\n",
      "*   **질의 최적화 (Query Optimization):** Naive RAG의 문제점을 해결하기 위해 질의 확장, 질의 변형, 질의 라우팅 등의 기법을 사용합니다.\n",
      "*   **임베딩 (Embedding):** 질문과 문서 청크 임베딩 간 유사도 계산 시 임베딩 모델의 의미 표현 능력이 중요하며, 혼합/하이브리드 검색, 임베딩 모델 파인튜닝 등의 방식이 활용됩니다.\n",
      "*   **어댑터 (Adapter):** 모델 파인튜닝 시 발생하는 문제를 해결하기 위해 외부 어댑터를 도입하여 정렬을 보조합니다.\n",
      "\n",
      "**4. Generation:**\n",
      "\n",
      "*   검색된 컨텍스트를 큐레이션하는 과정으로, Reranking을 통해 query와 관련성이 높은 chunk를 프롬프트의 앞 또는 뒤에 배치하고, Context Selection & Compression을 통해 불필요한 정보를 제거합니다.\n",
      "\n",
      "**5. Augmentation:**\n",
      "\n",
      "*   복잡한 추론 작업에서 성능을 향상시키기 위해 Iterative Retrieval, Recursive Retrieval, Adaptive Retrieval의 세 가지 검색 방식을 활용합니다. 이러한 방식들은 단일 단계 검색의 한계를 극복하며, 더 깊은 추론, 문제 해결, 효율적인 정보 활용을 가능하게 합니다.\n",
      "\n",
      "**핵심 내용:**\n",
      "\n",
      "*   RAG 기술은 LLM의 발전에 따라 진화해왔으며, 다양한 최적화 기법과 LLM과의 통합을 통해 성능을 향상시키고 있습니다.\n",
      "*   검색 단계의 최적화(인덱싱, 질의, 임베딩)와 생성 단계의 컨텍스트 큐레이션, 그리고 Augmentation 단계의 다양한 검색 방식 활용이 RAG 시스템의 핵심 요소입니다.\n",
      "*   RAG는 새로운 데이터와 지식을 LLM에 효과적으로 통합하는 데 중요한 역할을 수행합니다.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 159\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m▶ 질의를 입력하세요 (exit 입력 시 종료)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m질문: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m q\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m종료합니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# local_rag_modular.py\n",
    "\n",
    "import os\n",
    "from notion_client import Client\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from sentence_transformers import CrossEncoder\n",
    "import google.generativeai as genai\n",
    "\n",
    "# ─── 설정 ─────────────────────────────────────────────\n",
    "NOTION_API_KEY = \"ntn_S49134845636QN7OizYlyythCTORUXOCvYcp2U19S0P6dy\"\n",
    "PAGE_ID          = \"1dd124ddd31380acb247eb0c791331d9\"\n",
    "VECTOR_DB_DIR    = \"./vectordb\"\n",
    "EMBEDDING_MODEL  = \"jhgan/ko-sbert-sts\"\n",
    "RERANKER_MODEL   = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "GEMINI_MODEL     = \"gemini-2.0-flash\"\n",
    "GENAI_TEMP       = 1.0\n",
    "TOP_K            = 5\n",
    "\n",
    "# ─── Notion 클라이언트 초기화 ─────────────────────────\n",
    "os.environ[\"NOTION_API_KEY\"] = NOTION_API_KEY\n",
    "notion = Client(auth=NOTION_API_KEY)\n",
    "\n",
    "# ─── Google Generative AI 설정 ─────────────────────────\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\", \"\")\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# ─── Notion 로더 모듈 ─────────────────────────────────\n",
    "class NotionLoader:\n",
    "    def __init__(self, page_id: str, client: Client):\n",
    "        self.page_id = page_id\n",
    "        self.client = client\n",
    "\n",
    "    def fetch_blocks(self) -> list[dict]:\n",
    "        all_blocks = []\n",
    "        cursor = None\n",
    "        while True:\n",
    "            resp = self.client.blocks.children.list(\n",
    "                block_id=self.page_id,\n",
    "                start_cursor=cursor,\n",
    "                page_size=100\n",
    "            )\n",
    "            all_blocks.extend(resp[\"results\"])\n",
    "            if not resp.get(\"has_more\"):\n",
    "                break\n",
    "            cursor = resp.get(\"next_cursor\")\n",
    "        return all_blocks\n",
    "\n",
    "    def block_to_text(self, b: dict) -> str:\n",
    "        t = \"\"\n",
    "        typ = b[\"type\"]\n",
    "        rich = b[typ].get(\"rich_text\", [])\n",
    "        if typ in (\"paragraph\", \"heading_1\", \"heading_2\", \"heading_3\"):\n",
    "            for r in rich:\n",
    "                t += r.get(\"plain_text\", \"\")\n",
    "            if typ.startswith(\"heading\"):\n",
    "                t = \"\\n\" + t.upper() + \"\\n\"\n",
    "        elif typ == \"bulleted_list_item\":\n",
    "            for r in rich:\n",
    "                t += r.get(\"plain_text\", \"\")\n",
    "            t = \"- \" + t\n",
    "        elif typ == \"numbered_list_item\":\n",
    "            for r in rich:\n",
    "                t += r.get(\"plain_text\", \"\")\n",
    "            t = \"1. \" + t\n",
    "        elif typ == \"code\":\n",
    "            lang = b[\"code\"][\"language\"]\n",
    "            for r in rich:\n",
    "                t += r.get(\"plain_text\", \"\")\n",
    "            t = f\"```{lang}\\n{t}\\n```\"\n",
    "        return t.strip()\n",
    "\n",
    "    def to_documents(self) -> list[Document]:\n",
    "        blocks = self.fetch_blocks()\n",
    "        full_text = \"\\n\".join(\n",
    "            txt for blk in blocks\n",
    "            if (txt := self.block_to_text(blk))\n",
    "        )\n",
    "        return [Document(page_content=full_text, metadata={\"source\": self.page_id})]\n",
    "\n",
    "# ─── 임베딩 및 벡터스토어 모듈 ───────────────────────────\n",
    "class VectorModule:\n",
    "    def __init__(self, persist_dir: str, model_name: str):\n",
    "        self.persist_dir = persist_dir\n",
    "        self.embedding = SentenceTransformerEmbeddings(model_name=model_name)\n",
    "\n",
    "    def build_db(self, docs: list[Document]):\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=docs,\n",
    "            embedding=self.embedding,\n",
    "            persist_directory=self.persist_dir\n",
    "        )\n",
    "        vectordb.persist()\n",
    "\n",
    "    def load_db(self):\n",
    "        return Chroma(\n",
    "            persist_directory=self.persist_dir,\n",
    "            embedding_function=self.embedding\n",
    "        )\n",
    "\n",
    "# ─── 랭커 모듈 ─────────────────────────────────────────\n",
    "class Reranker:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.cross_encoder = CrossEncoder(model_name)\n",
    "\n",
    "    def rerank(self, docs: list[Document], query: str) -> list[Document]:\n",
    "        # CrossEncoder 점수를 기반으로 순위 재조정 (score 만으로 정렬)\n",
    "        pairs = [(query, doc.page_content) for doc in docs]\n",
    "        scores = self.cross_encoder.predict(pairs)\n",
    "        # scores 기준으로만 정렬하도록 key 지정\n",
    "        ranked_pairs = sorted(\n",
    "            zip(scores, docs),\n",
    "            key=lambda x: x[0],\n",
    "            reverse=True\n",
    "        )\n",
    "        return [doc for _, doc in ranked_pairs]\n",
    "\n",
    "# ─── 제너레이터 모듈 ────────────────────────────────────\n",
    "class GeminiGenerator:\n",
    "    def __init__(self, model: str, temperature: float):\n",
    "        self.model_name = model\n",
    "        self.temp = temperature\n",
    "\n",
    "    def generate(self, query: str, context: str) -> str:\n",
    "        prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "        model = genai.GenerativeModel(\n",
    "            self.model_name,\n",
    "            generation_config=genai.GenerationConfig(temperature=self.temp)\n",
    "        )\n",
    "        return model.generate_content(prompt).text\n",
    "\n",
    "# ─── 모듈러 RAG 파이프라인 ─────────────────────────────────\n",
    "class ModularRAG:\n",
    "    def __init__(self):\n",
    "        loader = NotionLoader(PAGE_ID, notion)\n",
    "        if not os.path.exists(VECTOR_DB_DIR):\n",
    "            docs = loader.to_documents()\n",
    "            VectorModule(VECTOR_DB_DIR, EMBEDDING_MODEL).build_db(docs)\n",
    "        self.db = VectorModule(VECTOR_DB_DIR, EMBEDDING_MODEL).load_db()\n",
    "        self.reranker = Reranker(RERANKER_MODEL)\n",
    "        self.generator = GeminiGenerator(GEMINI_MODEL, GENAI_TEMP)\n",
    "        self.top_k = TOP_K\n",
    "\n",
    "    def answer(self, query: str) -> str:\n",
    "        retriever = self.db.as_retriever(search_kwargs={\"k\": self.top_k * 2})\n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "        reranked = self.reranker.rerank(docs, query)\n",
    "        selected = reranked[: self.top_k]\n",
    "        context = \"\\n\\n\".join([d.page_content for d in selected])\n",
    "        return self.generator.generate(query, context)\n",
    "\n",
    "# ─── 대화형 실행 ───────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    rag = ModularRAG()\n",
    "    print(\"▶ 질의를 입력하세요 (exit 입력 시 종료)\")\n",
    "    while True:\n",
    "        q = input(\"질문: \").strip()\n",
    "        if q.lower() in (\"exit\", \"quit\"):\n",
    "            print(\"종료합니다.\")\n",
    "            break\n",
    "        print(\"\\n⏳ 답변 생성 중…\")\n",
    "        ans = rag.answer(q)\n",
    "        print(\"\\n💡 답변:\\n\", ans)\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39fb8063-63d2-4776-89e1-fab49229445b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_google_genai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m에러 발생: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_google_genai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerativeai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgenai\u001b[39;00m\n\u001b[0;32m     33\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatGoogleGenerativeAI(\n\u001b[0;32m     34\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-2.0-flash\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 또는 \"gemini-1.5-pro\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m     36\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_google_genai'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
