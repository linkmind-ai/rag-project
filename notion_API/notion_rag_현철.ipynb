{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c3ea616-282b-4396-a1e7-3ccd19a191d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement google-generative-ai (from versions: none)\n",
      "ERROR: No matching distribution found for google-generative-ai\n"
     ]
    }
   ],
   "source": [
    "!pip install notion-client langchain langchain-community chromadb sentence-transformers \\\n",
    "            google-generative-ai langchain-google-genai -q\n",
    "!pip install --upgrade --quiet google-generativeai langchain-google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a23cbb96-3edc-41df-bb49-24bb1ed3d2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "6f367f31-ca06-45f5-b8e1-248846542dbc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35f76081-c8fc-4b36-95ad-0427645aa89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Notion Î∏îÎ°ùÏùÑ Î°úÎìú Ï§ë...\n",
      "    ‚Üí Î°úÎìúÎêú Î∏îÎ°ù Í∞úÏàò: 127\n",
      "üîÑ Î∏îÎ°ù Ìï©Ï≥êÏÑú ÌÖçÏä§Ìä∏ Í∏∏Ïù¥: 3273\n",
      "üîÑ ÏûÑÎ≤†Îî© Î™®Îç∏ Ï¥àÍ∏∞Ìôî Ï§ë...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yss63\\AppData\\Local\\Temp\\ipykernel_13212\\3239570107.py:87: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = SentenceTransformerEmbeddings(model_name=\"jhgan/ko-sbert-sts\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Chroma Î≤°ÌÑ∞Ïä§ÌÜ†Ïñ¥Ïóê Ï†ÄÏû• Ï§ë...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yss63\\AppData\\Local\\Temp\\ipykernel_13212\\3239570107.py:95: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Î≤°ÌÑ∞ DB Ï†ÄÏû• ÏôÑÎ£å: ./vectordb\n",
      "\n",
      "‚ñ∂ ÏßàÏùòÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî (exit ÏûÖÎ†• Ïãú Ï¢ÖÎ£å)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÏßàÎ¨∏:  Ïù¥ Î¨∏ÏÑúÏóê ÎåÄÌï¥ ÏöîÏïΩÌï¥Ï§ò\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è≥ ÎãµÎ≥Ä ÏÉùÏÑ± Ï§ë‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yss63\\AppData\\Local\\Temp\\ipykernel_13212\\3239570107.py:110: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=VECTOR_DB_DIR, embedding_function=embedding)\n",
      "C:\\Users\\yss63\\AppData\\Local\\Temp\\ipykernel_13212\\3239570107.py:112: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° ÎãµÎ≥Ä:\n",
      "Ïù¥ Î¨∏ÏÑúÎäî RAG(Retrieval-Augmented Generation) Í∏∞Ïà†Ïùò Î∞úÏ†Ñ Í≥ºÏ†ïÍ≥º Íµ¨ÏÑ± ÏöîÏÜå, Í∑∏Î¶¨Í≥† Í¥ÄÎ†®Îêú Îã§ÏñëÌïú ÏµúÏ†ÅÌôî Í∏∞Î≤ïÎì§ÏùÑ Ï≤¥Í≥ÑÏ†ÅÏúºÎ°ú Î∂ÑÏÑùÌïòÍ≥† ÏûàÏäµÎãàÎã§.\n",
      "\n",
      "**1. RAG Í∏∞Ïà† Î∞úÏ†Ñ Îã®Í≥Ñ:**\n",
      "\n",
      "*   **1Îã®Í≥Ñ:** Transformer Î™®Îç∏Í≥º Ïô∏Î∂Ä ÏßÄÏãù Í≤∞Ìï©ÏùÑ ÌÜµÌïú ÏÑ±Îä• Ìñ•ÏÉÅ\n",
      "*   **2Îã®Í≥Ñ:** ChatGPT Îì±Ïû•ÏúºÎ°ú LLMÏùò In-Context Learning Îä•Î†• Í∞ïÌôî Î∞è RAGÎ•º ÌÜµÌïú Î≥µÏû°Ìïú ÏßàÏùò ÏùëÎãµ\n",
      "*   **3Îã®Í≥Ñ:** RAGÏôÄ LLMÏùò Íµ¨Ï°∞Ï†Å Í≤∞Ìï© Î∞è ÌååÏù∏ÌäúÎãù ÌÜµÌï©\n",
      "\n",
      "**2. RAG Í∞úÏöî:**\n",
      "\n",
      "*   **Í∏∞Î≥∏ ÏõêÎ¶¨:** Indexing -> Retrieval -> Generation Ïùò 3Îã®Í≥ÑÎ°ú Íµ¨ÏÑ±.\n",
      "*   **Naive RAG:** Í∞ÑÎã®Ìïú \"Retrieve-Read\" ÌååÏù¥ÌîÑÎùºÏù∏Ïù¥ÏßÄÎßå Îã®Ï†ê Ï°¥Ïû¨.\n",
      "*   **Advanced RAG:** Í≤ÄÏÉâ ÌíàÏßà Í∞úÏÑ†ÏùÑ ÏúÑÌïú pre-retrieval Î∞è post-retrieval ÏµúÏ†ÅÌôî Ï†ÅÏö©.\n",
      "*   **Modular RAG:** ÏÉàÎ°úÏö¥ Î™®ÎìàÍ≥º ÏÉÅÌò∏ÏûëÏö© Ìå®ÌÑ¥ÏùÑ Ï∂îÍ∞ÄÌïòÏó¨ Ïú†Ïó∞ÏÑ± Î∞è ÎßûÏ∂§Ìôî Í∞ïÌôî.\n",
      "*   **RAG vs Fine-tuning:** RAGÎäî ÏÉàÎ°úÏö¥ Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ Î∞è Fine-tuningÍ≥º Í≤∞Ìï© Ïãú ÏÑ±Îä• Ìñ•ÏÉÅ.\n",
      "\n",
      "**3. Retrieval (Í≤ÄÏÉâ) Ï£ºÏöî ÎÇ¥Ïö©:**\n",
      "\n",
      "*   **Retrieval Source:** Îã§ÏñëÌïú ÌòïÌÉúÏùò Îç∞Ïù¥ÌÑ∞ (ÎπÑÏ†ïÌòï, Î∞òÏ†ïÌòï, Íµ¨Ï°∞Ìôî, LLM ÏÉùÏÑ± ÏΩòÌÖêÏ∏†) ÌôúÏö©\n",
      "*   **Retrieval Granularity:** ÏÇ¨Ïö©Ïûê PromptÏóê Îî∞Îùº Í≤ÄÏÉâ Îã®ÏúÑ ÏÑ∏Î∂ÑÏÑ± ÏÑ§Ï†ï Ï§ëÏöî (Ï†ïÌôïÎèÑ Ìñ•ÏÉÅ).\n",
      "*   **Indexing Optimization:** Ï≤≠ÌÅ¨ Î∂ÑÌï† Ï†ÑÎûµ, Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î∂ÄÏ∞©, Íµ¨Ï°∞Ï†Å Ïù∏Îç±Ïä§ Íµ¨Ï∂ïÏùÑ ÌÜµÌï¥ Í≤ÄÏÉâ Ìö®Í≥º Ìñ•ÏÉÅ.\n",
      "*   **Query Optimization:** ÏßàÏùò ÌôïÏû•, Î≥ÄÌòï, ÎùºÏö∞ÌåÖÏùÑ ÌÜµÌï¥ Naive RAGÏùò Î¨∏Ï†úÏ†ê Í∞úÏÑ†.\n",
      "*   **Embedding:** ÏûÑÎ≤†Îî© Î™®Îç∏Ïùò ÏùòÎØ∏ ÌëúÌòÑ Îä•Î†• Ï§ëÏöî, ÌòºÌï© Í≤ÄÏÉâ Î∞è ÏûÑÎ≤†Îî© Î™®Îç∏ ÌååÏù∏ÌäúÎãù ÌôúÏö©.\n",
      "*   **Adapter:** Î™®Îç∏ ÌååÏù∏ÌäúÎãù Ïãú Î∞úÏÉùÌïòÎäî Î¨∏Ï†ú Ìï¥Í≤∞ÏùÑ ÏúÑÌï¥ Ïô∏Î∂Ä Ïñ¥ÎåëÌÑ∞ ÎèÑÏûÖ.\n",
      "\n",
      "**4. Generation (ÏÉùÏÑ±) Î∞è Augmentation (ÌôïÏû•):**\n",
      "\n",
      "*   **Context Curation:** Reranking (Ïû¨Ï†ïÎ†¨), Context Selection & Compression Îì±ÏùÑ ÌÜµÌï¥ LLM ÏûÖÎ†• ÏµúÏ†ÅÌôî.\n",
      "*   **Augmentation:** Iterative, Recursive, Adaptive Retrieval Î∞©ÏãùÏùÑ ÌôúÏö©ÌïòÏó¨ Î≥µÏû°Ìïú Ï∂îÎ°† ÏûëÏóÖ ÏÑ±Îä• Ìñ•ÏÉÅ.\n",
      "\n",
      "**Í≤∞Î°†Ï†ÅÏúºÎ°ú, Ïù¥ Î¨∏ÏÑúÎäî RAG Í∏∞Ïà†Ïùò Î∞úÏ†Ñ, Íµ¨ÏÑ± ÏöîÏÜå, Í∑∏Î¶¨Í≥† ÏÑ±Îä• Ìñ•ÏÉÅÏùÑ ÏúÑÌïú Îã§ÏñëÌïú Î∞©Î≤ïÎ°†ÏùÑ Ï¢ÖÌï©Ï†ÅÏúºÎ°ú Ï†úÏãúÌïòÎ©∞, LLMÍ≥ºÏùò ÌÜµÌï©ÏùÑ Í∞ïÏ°∞ÌïòÍ≥† ÏûàÏäµÎãàÎã§.**\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 120\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚ñ∂ ÏßàÏùòÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî (exit ÏûÖÎ†• Ïãú Ï¢ÖÎ£å)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mÏßàÎ¨∏: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m q\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mÏ¢ÖÎ£åÌï©ÎãàÎã§.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# local_rag_gemini.py\n",
    "\n",
    "import os\n",
    "from notion_client import Client\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import google.generativeai as genai\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ ÏÇ¨Ïö©Ïûê ÏÑ§Ï†ï ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "NOTION_API_KEY = \"ntn_S49134845636QN7OizYlyythCTORUXOCvYcp2U19S0P6dy\"        # Ïòà: ntn_...\n",
    "PAGE_ID          = \"1dd124ddd31380acb247eb0c791331d9\"                       # ?pvs=11 Ï†úÍ±∞Îêú ÏàúÏàò ÌéòÏù¥ÏßÄ ID\n",
    "VECTOR_DB_DIR    = \"./vectordb\"                         # Î≤°ÌÑ∞ DBÍ∞Ä Ï†ÄÏû•Îê† ÎîîÎ†âÌÜ†Î¶¨\n",
    "GOOGLE_API_KEY   = \"AIzaSyDSQvjXRYlqA6nYdCSX8l6WmoZxXV0aHMo\"                # Google Generative AI ÌÇ§\n",
    "GEMINI_MODEL     = \"gemini-2.0-flash\"                   # or \"gemini-1.5-pro\"\n",
    "GENAI_TEMP       = 1.0                                  # ÏÉùÏÑ± Ïò®ÎèÑ\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Notion ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÑ§Ï†ï ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "os.environ[\"NOTION_API_KEY\"] = NOTION_API_KEY\n",
    "notion = Client(auth=NOTION_API_KEY)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Google Generative AI ÏÑ§Ï†ï ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "# (LangChain ÎûòÌçºÎäî Ïì∞ÏßÄ ÏïäÍ≥†, ÏßÅÏ†ë genai.GenerativeModel Ìò∏Ï∂ú)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 1) Notion ÌéòÏù¥ÏßÄ Î∏îÎ°ù Ï†ÑÎ∂Ä Í∞ÄÏ†∏Ïò§Í∏∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def fetch_all_blocks(page_id: str) -> list[dict]:\n",
    "    all_blocks = []\n",
    "    cursor = None\n",
    "    while True:\n",
    "        resp = notion.blocks.children.list(\n",
    "            block_id=page_id,\n",
    "            start_cursor=cursor,\n",
    "            page_size=100\n",
    "        )\n",
    "        all_blocks.extend(resp[\"results\"])\n",
    "        if not resp.get(\"has_more\"):\n",
    "            break\n",
    "        cursor = resp.get(\"next_cursor\")\n",
    "    return all_blocks\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 2) Î∏îÎ°ùÏóêÏÑú ÌÖçÏä§Ìä∏Îßå ÎΩëÏïÑÎÇ¥Í∏∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def block_to_text(b: dict) -> str:\n",
    "    t = \"\"\n",
    "    typ = b[\"type\"]\n",
    "    rich = b[typ].get(\"rich_text\", [])\n",
    "    # Îã®Ïàú paragraph / heading / list / code Ï≤òÎ¶¨\n",
    "    if typ in (\"paragraph\", \"heading_1\", \"heading_2\", \"heading_3\"):\n",
    "        for r in rich:\n",
    "            t += r.get(\"plain_text\", \"\")\n",
    "        if typ.startswith(\"heading\"):\n",
    "            t = \"\\n\" + t.upper() + \"\\n\"\n",
    "    elif typ == \"bulleted_list_item\":\n",
    "        for r in rich:\n",
    "            t += r.get(\"plain_text\", \"\")\n",
    "        t = \"- \" + t\n",
    "    elif typ == \"numbered_list_item\":\n",
    "        for r in rich:\n",
    "            t += r.get(\"plain_text\", \"\")\n",
    "        t = \"1. \" + t\n",
    "    elif typ == \"code\":\n",
    "        lang = b[\"code\"][\"language\"]\n",
    "        for r in rich:\n",
    "            t += r.get(\"plain_text\", \"\")\n",
    "        t = f\"```{lang}\\n{t}\\n```\"\n",
    "    return t\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 3) ÌéòÏù¥ÏßÄ ÌïòÎÇòÎ•º LangChain DocumentÎ°ú Î¨∂Í∏∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"üîÑ Notion Î∏îÎ°ùÏùÑ Î°úÎìú Ï§ë...\")\n",
    "blocks = fetch_all_blocks(PAGE_ID)\n",
    "print(f\"    ‚Üí Î°úÎìúÎêú Î∏îÎ°ù Í∞úÏàò: {len(blocks)}\")\n",
    "\n",
    "full_text = \"\\n\".join(\n",
    "    txt for blk in blocks\n",
    "    if (txt := block_to_text(blk).strip())\n",
    ")\n",
    "print(f\"üîÑ Î∏îÎ°ù Ìï©Ï≥êÏÑú ÌÖçÏä§Ìä∏ Í∏∏Ïù¥: {len(full_text)}\")\n",
    "\n",
    "docs = [Document(page_content=full_text, metadata={\"source\": PAGE_ID})]\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 4) SBERT ÏûÑÎ≤†Îî© + Chroma DB ÏÉùÏÑ± ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"üîÑ ÏûÑÎ≤†Îî© Î™®Îç∏ Ï¥àÍ∏∞Ìôî Ï§ë...\")\n",
    "embedding = SentenceTransformerEmbeddings(model_name=\"jhgan/ko-sbert-sts\")\n",
    "\n",
    "print(\"üîÑ Chroma Î≤°ÌÑ∞Ïä§ÌÜ†Ïñ¥Ïóê Ï†ÄÏû• Ï§ë...\")\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding,\n",
    "    persist_directory=VECTOR_DB_DIR\n",
    ")\n",
    "vectordb.persist()\n",
    "print(\"‚úÖ Î≤°ÌÑ∞ DB Ï†ÄÏû• ÏôÑÎ£å:\", VECTOR_DB_DIR)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 5) GeminiÎ•º Ïù¥Ïö©Ìïú RetrievalQA Ìó¨Ìçº ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def generate_gemini_answer(query: str, context: str) -> str:\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "    model = genai.GenerativeModel(\n",
    "        GEMINI_MODEL,\n",
    "        generation_config=genai.GenerationConfig(temperature=GENAI_TEMP)\n",
    "    )\n",
    "    return model.generate_content(prompt).text\n",
    "\n",
    "\n",
    "def ask_question(q: str) -> str:\n",
    "    # ÏµúÏã† DB Î°úÎìú\n",
    "    db = Chroma(persist_directory=VECTOR_DB_DIR, embedding_function=embedding)\n",
    "    retriever = db.as_retriever()\n",
    "    docs = retriever.get_relevant_documents(q)\n",
    "    context = \"\\n\\n\".join(d.page_content for d in docs)\n",
    "    return generate_gemini_answer(q, context)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 6) ÎåÄÌôîÌòï Î£®ÌîÑ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n‚ñ∂ ÏßàÏùòÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî (exit ÏûÖÎ†• Ïãú Ï¢ÖÎ£å)\")\n",
    "    while True:\n",
    "        q = input(\"ÏßàÎ¨∏: \").strip()\n",
    "        if q.lower() in (\"exit\",\"quit\"):\n",
    "            print(\"Ï¢ÖÎ£åÌï©ÎãàÎã§.\")\n",
    "            break\n",
    "        print(\"\\n‚è≥ ÎãµÎ≥Ä ÏÉùÏÑ± Ï§ë‚Ä¶\")\n",
    "        ans = ask_question(q)\n",
    "        print(\"\\nüí° ÎãµÎ≥Ä:\")\n",
    "        print(ans)\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bb8dc36-7d15-48f1-ab8e-c58d190571e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ ÏßàÏùòÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî (exit ÏûÖÎ†• Ïãú Ï¢ÖÎ£å)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÏßàÎ¨∏:  Ïù¥ Î¨∏ÏÑúÏóê ÎåÄÌï¥ ÏöîÏïΩÌï¥Ï§ò\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è≥ ÎãµÎ≥Ä ÏÉùÏÑ± Ï§ë‚Ä¶\n",
      "\n",
      "üí° ÎãµÎ≥Ä:\n",
      " ## RAG Í∏∞Ïà† Î∞úÏ†Ñ Î∞è LLM ÌÜµÌï© Ïó∞Íµ¨ ÎÖºÎ¨∏ ÏöîÏïΩ\n",
      "\n",
      "**1. ÏÑúÎ°†:**\n",
      "\n",
      "*   Î≥∏ ÎÖºÎ¨∏ÏùÄ RAG(Retrieval-Augmented Generation) Í∏∞Ïà†Ïùò Î∞úÏ†Ñ Í≥ºÏ†ïÏùÑ Ï≤¥Í≥ÑÏ†ÅÏúºÎ°ú Ï†ïÎ¶¨ÌïòÍ≥†, LLM(Large Language Model)Í≥ºÏùò ÌÜµÌï©ÏùÑ Ï§ëÏã¨ÏúºÎ°ú Í∏∞Ïà† Ìå®Îü¨Îã§ÏûÑÍ≥º Ïó∞Íµ¨ Î∞©Î≤ïÎ°†ÏùÑ Î∂ÑÏÑùÌï©ÎãàÎã§.\n",
      "*   RAG Í∏∞Ïà†ÏùÄ Transformer Î™®Îç∏Ïùò Îì±Ïû•ÏúºÎ°ú ÏãúÏûëÎêòÏñ¥ ChatGPTÏôÄ Í∞ôÏùÄ LLMÏùò Î∞úÏ†ÑÍ≥º Ìï®Íªò Í≥†ÎèÑÌôîÎêòÏóàÏúºÎ©∞, ÌòÑÏû¨Îäî Îã®Ïàú Í≤ÄÏÉâ Î≥¥Ï°∞Î•º ÎÑòÏñ¥ LLM ÌååÏù∏ÌäúÎãù Î∞è Íµ¨Ï°∞Ï†Å Í≤∞Ìï©ÏùÑ ÌÜµÌï¥ ÎçîÏö± Î∞úÏ†ÑÌïòÍ≥† ÏûàÏäµÎãàÎã§.\n",
      "\n",
      "**2. RAG Í∞úÏöî:**\n",
      "\n",
      "*   RAGÎäî ÏßàÎ¨∏Ïóê ÎãµÎ≥ÄÌïòÍ∏∞ ÏúÑÌï¥ Ïô∏Î∂Ä ÏßÄÏãùÏùÑ ÌôúÏö©ÌïòÎäî Î∞©ÏãùÏúºÎ°ú, ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1) Indexing, 2) Retrieval, 3) GenerationÏùò ÏÑ∏ Îã®Í≥ÑÎ•º Í±∞Ïπ©ÎãàÎã§.\n",
      "*   RAGÎäî Naive RAGÏóêÏÑú Advanced RAG, Modular RAGÎ°ú Î∞úÏ†ÑÌï¥ÏôîÏúºÎ©∞, Fine-Tuning(FT)Í≥º Í≤∞Ìï©ÌïòÏó¨ ÏÑ±Îä•ÏùÑ ÎçîÏö± Ìñ•ÏÉÅÏãúÌÇ¨ Ïàò ÏûàÏäµÎãàÎã§. RAGÎäî ÌäπÌûà ÏÉàÎ°úÏö¥ Îç∞Ïù¥ÌÑ∞ÎÇò Î≥¥ÏßÄ Î™ªÌïú ÏßÄÏãùÏùÑ Ï≤òÎ¶¨ÌïòÎäî Îç∞ FTÎ≥¥Îã§ Ïö∞ÏàòÌïú ÏÑ±Îä•ÏùÑ Î≥¥ÏûÖÎãàÎã§.\n",
      "\n",
      "**3. Retrieval:**\n",
      "\n",
      "*   Retrieval Îã®Í≥ÑÎäî Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§Î°úÎ∂ÄÌÑ∞ Í¥ÄÎ†® Î¨∏ÏÑúÎ•º Ìö®Ïú®Ï†ÅÏúºÎ°ú Í≤ÄÏÉâÌïòÎäî Í≤ÉÏù¥ ÌïµÏã¨ÏûÖÎãàÎã§.\n",
      "*   **Í≤ÄÏÉâ ÏÜåÏä§ (Retrieval Source):** RAGÎäî LLMÏùÑ Î≥¥ÏôÑÌïòÍ∏∞ ÏúÑÌï¥ Ïô∏Î∂Ä ÏßÄÏãùÏóê ÏùòÏ°¥ÌïòÎ©∞, Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞Îäî ÎπÑÏ†ïÌòï, Î∞òÍµ¨Ï°∞Ìôî, Íµ¨Ï°∞Ìôî Îç∞Ïù¥ÌÑ∞ Î∞è LLM ÏÉùÏÑ± ÏΩòÌÖêÏ∏†Î•º Ìè¨Ìï®Ìï©ÎãàÎã§. Í≤ÄÏÉâ Îã®ÏúÑ ÏÑ∏Î∂ÑÏÑ±ÏùÄ ÏÇ¨Ïö©Ïûê PromptÏóê Îî∞Îùº ÏÑ§Ï†ïÌïòÏó¨ Í≤ÄÏÉâ Ï†ïÌôïÎèÑÎ•º ÎÜíÏûÖÎãàÎã§.\n",
      "*   **Ïù∏Îç±Ïã± ÏµúÏ†ÅÌôî (Indexing Optimization):** Î¨∏ÏÑú Ï†ÑÏ≤òÎ¶¨, Î∂ÑÌï†(Chunking), ÏûÑÎ≤†Îî© Î≥ÄÌôò ÌõÑ Î≤°ÌÑ∞ DBÏóê Ï†ÄÏû•ÌïòÎäî Í≥ºÏ†ïÏùÑ ÌÜµÌï¥ Í≤ÄÏÉâ Ìö®Í≥ºÎ•º Ìñ•ÏÉÅÏãúÌÇµÎãàÎã§. Ï≤≠ÌÅ¨ Î∂ÑÌï† Ï†ÑÎûµ, Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î∂ÄÏ∞©, Íµ¨Ï°∞Ï†Å Ïù∏Îç±Ïä§ Îì±Ïù¥ ÌôúÏö©Îê©ÎãàÎã§.\n",
      "*   **ÏßàÏùò ÏµúÏ†ÅÌôî (Query Optimization):** Naive RAGÏùò Î¨∏Ï†úÏ†êÏùÑ Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ ÏßàÏùò ÌôïÏû•, ÏßàÏùò Î≥ÄÌòï, ÏßàÏùò ÎùºÏö∞ÌåÖ Îì±Ïùò Í∏∞Î≤ïÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n",
      "*   **ÏûÑÎ≤†Îî© (Embedding):** ÏßàÎ¨∏Í≥º Î¨∏ÏÑú Ï≤≠ÌÅ¨ ÏûÑÎ≤†Îî© Í∞Ñ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞ Ïãú ÏûÑÎ≤†Îî© Î™®Îç∏Ïùò ÏùòÎØ∏ ÌëúÌòÑ Îä•Î†•Ïù¥ Ï§ëÏöîÌïòÎ©∞, ÌòºÌï©/ÌïòÏù¥Î∏åÎ¶¨Îìú Í≤ÄÏÉâ, ÏûÑÎ≤†Îî© Î™®Îç∏ ÌååÏù∏ÌäúÎãù Îì±Ïùò Î∞©ÏãùÏù¥ ÌôúÏö©Îê©ÎãàÎã§.\n",
      "*   **Ïñ¥ÎåëÌÑ∞ (Adapter):** Î™®Îç∏ ÌååÏù∏ÌäúÎãù Ïãú Î∞úÏÉùÌïòÎäî Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ Ïô∏Î∂Ä Ïñ¥ÎåëÌÑ∞Î•º ÎèÑÏûÖÌïòÏó¨ Ï†ïÎ†¨ÏùÑ Î≥¥Ï°∞Ìï©ÎãàÎã§.\n",
      "\n",
      "**4. Generation:**\n",
      "\n",
      "*   Í≤ÄÏÉâÎêú Ïª®ÌÖçÏä§Ìä∏Î•º ÌÅêÎ†àÏù¥ÏÖòÌïòÎäî Í≥ºÏ†ïÏúºÎ°ú, RerankingÏùÑ ÌÜµÌï¥ queryÏôÄ Í¥ÄÎ†®ÏÑ±Ïù¥ ÎÜíÏùÄ chunkÎ•º ÌîÑÎ°¨ÌîÑÌä∏Ïùò Ïïû ÎòêÎäî Îí§Ïóê Î∞∞ÏπòÌïòÍ≥†, Context Selection & CompressionÏùÑ ÌÜµÌï¥ Î∂àÌïÑÏöîÌïú Ï†ïÎ≥¥Î•º Ï†úÍ±∞Ìï©ÎãàÎã§.\n",
      "\n",
      "**5. Augmentation:**\n",
      "\n",
      "*   Î≥µÏû°Ìïú Ï∂îÎ°† ÏûëÏóÖÏóêÏÑú ÏÑ±Îä•ÏùÑ Ìñ•ÏÉÅÏãúÌÇ§Í∏∞ ÏúÑÌï¥ Iterative Retrieval, Recursive Retrieval, Adaptive RetrievalÏùò ÏÑ∏ Í∞ÄÏßÄ Í≤ÄÏÉâ Î∞©ÏãùÏùÑ ÌôúÏö©Ìï©ÎãàÎã§. Ïù¥Îü¨Ìïú Î∞©ÏãùÎì§ÏùÄ Îã®Ïùº Îã®Í≥Ñ Í≤ÄÏÉâÏùò ÌïúÍ≥ÑÎ•º Í∑πÎ≥µÌïòÎ©∞, Îçî ÍπäÏùÄ Ï∂îÎ°†, Î¨∏Ï†ú Ìï¥Í≤∞, Ìö®Ïú®Ï†ÅÏù∏ Ï†ïÎ≥¥ ÌôúÏö©ÏùÑ Í∞ÄÎä•ÌïòÍ≤å Ìï©ÎãàÎã§.\n",
      "\n",
      "**ÌïµÏã¨ ÎÇ¥Ïö©:**\n",
      "\n",
      "*   RAG Í∏∞Ïà†ÏùÄ LLMÏùò Î∞úÏ†ÑÏóê Îî∞Îùº ÏßÑÌôîÌï¥ÏôîÏúºÎ©∞, Îã§ÏñëÌïú ÏµúÏ†ÅÌôî Í∏∞Î≤ïÍ≥º LLMÍ≥ºÏùò ÌÜµÌï©ÏùÑ ÌÜµÌï¥ ÏÑ±Îä•ÏùÑ Ìñ•ÏÉÅÏãúÌÇ§Í≥† ÏûàÏäµÎãàÎã§.\n",
      "*   Í≤ÄÏÉâ Îã®Í≥ÑÏùò ÏµúÏ†ÅÌôî(Ïù∏Îç±Ïã±, ÏßàÏùò, ÏûÑÎ≤†Îî©)ÏôÄ ÏÉùÏÑ± Îã®Í≥ÑÏùò Ïª®ÌÖçÏä§Ìä∏ ÌÅêÎ†àÏù¥ÏÖò, Í∑∏Î¶¨Í≥† Augmentation Îã®Í≥ÑÏùò Îã§ÏñëÌïú Í≤ÄÏÉâ Î∞©Ïãù ÌôúÏö©Ïù¥ RAG ÏãúÏä§ÌÖúÏùò ÌïµÏã¨ ÏöîÏÜåÏûÖÎãàÎã§.\n",
      "*   RAGÎäî ÏÉàÎ°úÏö¥ Îç∞Ïù¥ÌÑ∞ÏôÄ ÏßÄÏãùÏùÑ LLMÏóê Ìö®Í≥ºÏ†ÅÏúºÎ°ú ÌÜµÌï©ÌïòÎäî Îç∞ Ï§ëÏöîÌïú Ïó≠Ìï†ÏùÑ ÏàòÌñâÌï©ÎãàÎã§.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 159\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ñ∂ ÏßàÏùòÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî (exit ÏûÖÎ†• Ïãú Ï¢ÖÎ£å)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mÏßàÎ¨∏: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m q\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mÏ¢ÖÎ£åÌï©ÎãàÎã§.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# local_rag_modular.py\n",
    "\n",
    "import os\n",
    "from notion_client import Client\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from sentence_transformers import CrossEncoder\n",
    "import google.generativeai as genai\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ ÏÑ§Ï†ï ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "NOTION_API_KEY = \"ntn_S49134845636QN7OizYlyythCTORUXOCvYcp2U19S0P6dy\"\n",
    "PAGE_ID          = \"1dd124ddd31380acb247eb0c791331d9\"\n",
    "VECTOR_DB_DIR    = \"./vectordb\"\n",
    "EMBEDDING_MODEL  = \"jhgan/ko-sbert-sts\"\n",
    "RERANKER_MODEL   = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "GEMINI_MODEL     = \"gemini-2.0-flash\"\n",
    "GENAI_TEMP       = 1.0\n",
    "TOP_K            = 5\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Notion ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "os.environ[\"NOTION_API_KEY\"] = NOTION_API_KEY\n",
    "notion = Client(auth=NOTION_API_KEY)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Google Generative AI ÏÑ§Ï†ï ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\", \"\")\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Notion Î°úÎçî Î™®Îìà ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class NotionLoader:\n",
    "    def __init__(self, page_id: str, client: Client):\n",
    "        self.page_id = page_id\n",
    "        self.client = client\n",
    "\n",
    "    def fetch_blocks(self) -> list[dict]:\n",
    "        all_blocks = []\n",
    "        cursor = None\n",
    "        while True:\n",
    "            resp = self.client.blocks.children.list(\n",
    "                block_id=self.page_id,\n",
    "                start_cursor=cursor,\n",
    "                page_size=100\n",
    "            )\n",
    "            all_blocks.extend(resp[\"results\"])\n",
    "            if not resp.get(\"has_more\"):\n",
    "                break\n",
    "            cursor = resp.get(\"next_cursor\")\n",
    "        return all_blocks\n",
    "\n",
    "    def block_to_text(self, b: dict) -> str:\n",
    "        t = \"\"\n",
    "        typ = b[\"type\"]\n",
    "        rich = b[typ].get(\"rich_text\", [])\n",
    "        if typ in (\"paragraph\", \"heading_1\", \"heading_2\", \"heading_3\"):\n",
    "            for r in rich:\n",
    "                t += r.get(\"plain_text\", \"\")\n",
    "            if typ.startswith(\"heading\"):\n",
    "                t = \"\\n\" + t.upper() + \"\\n\"\n",
    "        elif typ == \"bulleted_list_item\":\n",
    "            for r in rich:\n",
    "                t += r.get(\"plain_text\", \"\")\n",
    "            t = \"- \" + t\n",
    "        elif typ == \"numbered_list_item\":\n",
    "            for r in rich:\n",
    "                t += r.get(\"plain_text\", \"\")\n",
    "            t = \"1. \" + t\n",
    "        elif typ == \"code\":\n",
    "            lang = b[\"code\"][\"language\"]\n",
    "            for r in rich:\n",
    "                t += r.get(\"plain_text\", \"\")\n",
    "            t = f\"```{lang}\\n{t}\\n```\"\n",
    "        return t.strip()\n",
    "\n",
    "    def to_documents(self) -> list[Document]:\n",
    "        blocks = self.fetch_blocks()\n",
    "        full_text = \"\\n\".join(\n",
    "            txt for blk in blocks\n",
    "            if (txt := self.block_to_text(blk))\n",
    "        )\n",
    "        return [Document(page_content=full_text, metadata={\"source\": self.page_id})]\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ ÏûÑÎ≤†Îî© Î∞è Î≤°ÌÑ∞Ïä§ÌÜ†Ïñ¥ Î™®Îìà ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class VectorModule:\n",
    "    def __init__(self, persist_dir: str, model_name: str):\n",
    "        self.persist_dir = persist_dir\n",
    "        self.embedding = SentenceTransformerEmbeddings(model_name=model_name)\n",
    "\n",
    "    def build_db(self, docs: list[Document]):\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=docs,\n",
    "            embedding=self.embedding,\n",
    "            persist_directory=self.persist_dir\n",
    "        )\n",
    "        vectordb.persist()\n",
    "\n",
    "    def load_db(self):\n",
    "        return Chroma(\n",
    "            persist_directory=self.persist_dir,\n",
    "            embedding_function=self.embedding\n",
    "        )\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Îû≠Ïª§ Î™®Îìà ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class Reranker:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.cross_encoder = CrossEncoder(model_name)\n",
    "\n",
    "    def rerank(self, docs: list[Document], query: str) -> list[Document]:\n",
    "        # CrossEncoder Ï†êÏàòÎ•º Í∏∞Î∞òÏúºÎ°ú ÏàúÏúÑ Ïû¨Ï°∞Ï†ï (score ÎßåÏúºÎ°ú Ï†ïÎ†¨)\n",
    "        pairs = [(query, doc.page_content) for doc in docs]\n",
    "        scores = self.cross_encoder.predict(pairs)\n",
    "        # scores Í∏∞Ï§ÄÏúºÎ°úÎßå Ï†ïÎ†¨ÌïòÎèÑÎ°ù key ÏßÄÏ†ï\n",
    "        ranked_pairs = sorted(\n",
    "            zip(scores, docs),\n",
    "            key=lambda x: x[0],\n",
    "            reverse=True\n",
    "        )\n",
    "        return [doc for _, doc in ranked_pairs]\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Ï†úÎÑàÎ†àÏù¥ÌÑ∞ Î™®Îìà ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class GeminiGenerator:\n",
    "    def __init__(self, model: str, temperature: float):\n",
    "        self.model_name = model\n",
    "        self.temp = temperature\n",
    "\n",
    "    def generate(self, query: str, context: str) -> str:\n",
    "        prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "        model = genai.GenerativeModel(\n",
    "            self.model_name,\n",
    "            generation_config=genai.GenerationConfig(temperature=self.temp)\n",
    "        )\n",
    "        return model.generate_content(prompt).text\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Î™®ÎìàÎü¨ RAG ÌååÏù¥ÌîÑÎùºÏù∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class ModularRAG:\n",
    "    def __init__(self):\n",
    "        loader = NotionLoader(PAGE_ID, notion)\n",
    "        if not os.path.exists(VECTOR_DB_DIR):\n",
    "            docs = loader.to_documents()\n",
    "            VectorModule(VECTOR_DB_DIR, EMBEDDING_MODEL).build_db(docs)\n",
    "        self.db = VectorModule(VECTOR_DB_DIR, EMBEDDING_MODEL).load_db()\n",
    "        self.reranker = Reranker(RERANKER_MODEL)\n",
    "        self.generator = GeminiGenerator(GEMINI_MODEL, GENAI_TEMP)\n",
    "        self.top_k = TOP_K\n",
    "\n",
    "    def answer(self, query: str) -> str:\n",
    "        retriever = self.db.as_retriever(search_kwargs={\"k\": self.top_k * 2})\n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "        reranked = self.reranker.rerank(docs, query)\n",
    "        selected = reranked[: self.top_k]\n",
    "        context = \"\\n\\n\".join([d.page_content for d in selected])\n",
    "        return self.generator.generate(query, context)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ ÎåÄÌôîÌòï Ïã§Ìñâ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if __name__ == \"__main__\":\n",
    "    rag = ModularRAG()\n",
    "    print(\"‚ñ∂ ÏßàÏùòÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî (exit ÏûÖÎ†• Ïãú Ï¢ÖÎ£å)\")\n",
    "    while True:\n",
    "        q = input(\"ÏßàÎ¨∏: \").strip()\n",
    "        if q.lower() in (\"exit\", \"quit\"):\n",
    "            print(\"Ï¢ÖÎ£åÌï©ÎãàÎã§.\")\n",
    "            break\n",
    "        print(\"\\n‚è≥ ÎãµÎ≥Ä ÏÉùÏÑ± Ï§ë‚Ä¶\")\n",
    "        ans = rag.answer(q)\n",
    "        print(\"\\nüí° ÎãµÎ≥Ä:\\n\", ans)\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39fb8063-63d2-4776-89e1-fab49229445b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_google_genai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mÏóêÎü¨ Î∞úÏÉù: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_google_genai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerativeai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgenai\u001b[39;00m\n\u001b[0;32m     33\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatGoogleGenerativeAI(\n\u001b[0;32m     34\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-2.0-flash\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# ÎòêÎäî \"gemini-1.5-pro\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m     36\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_google_genai'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
